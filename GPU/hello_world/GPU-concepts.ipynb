{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c05393",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## GPU Concepts\n",
    "\n",
    "Burton Rosenberg\n",
    "\n",
    "29 May 2023\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "The NVidia GPU supports a programming library called CUDA. This adapts the GPU's graphic rendering hardware to the needs of scientific computation. Here we introduce how the architecture of an NVidia GPU.\n",
    "\n",
    "### Heterogeneous computing \n",
    "\n",
    "The NVidia GPU is an example of _heterogeneous computing_. The GPU is a device on the sytems bus that does a different style of computing than the CPU. Those things more appropriate to a GPU are shipped to the GPU for computing. \n",
    "\n",
    "This includes,\n",
    "\n",
    "- data movement from the CPU memory to the GPU memory\n",
    "- the code, a kernel, sent to the GPU \n",
    "- the kernel launched and the computation undertaken\n",
    "- the result of the computation copied from the GPU memory to the CPU memory.\n",
    "\n",
    "### Single Instruction Multiple Data (SIMD) parallelism\n",
    "\n",
    "While NVidia calls its computing SIMT (Single Instruction Multiple Thread), for simplicity will look at how it implements a modified of Single Instruction Multile Data parallelism.\n",
    "\n",
    "In strict SIMD, a single program works lock-step, instruction by instruction, and only the data worked upon is different. The program is called the _kernel_ and might look like,\n",
    "<pre>\n",
    "    __global__ void sum_array(float * a, float *b, float * c) {\n",
    "        int i = threadIdx.x + blockIdx.x * blockDim.x ;\n",
    "        a[i] = b[i] + c[i] ;\n",
    "        return ;\n",
    "    }\n",
    "</pre>\n",
    "Except for the index `i`, which is computed from some magically present constants `threadIdx`, `blockIdx` and `blockDim`, all threads run exactly this code in lock step.\n",
    "\n",
    "True SIMD would not allow any instance in the kernel to diverge. NVidia does allow a kernel to have loops and branches that depend on the code values. Threads are launced in groups of 32 called a _warp_. Different warps proceed completely independently. They might be launched at different times, and will finish at different times.\n",
    "\n",
    "\n",
    "Within a warp all threads work instruction by instruction lock-step. If there is a branch and some threads in the warp go one side of the branch, and some the other, in fact every thread runs both sides of the branches. However, the result of the execution will be disabled for the wrong side of the branch, per thread. This is called _warp divergence_.\n",
    "\n",
    "### Streaming Multiprocessors\n",
    "\n",
    "The execution machinery of an NVidia GPU is comprised of repeating Streaming Mutliprocessor (SM) units. \n",
    "Each contain cores for such operations as floating point and tensor arithmetic. Each SM can launch a \n",
    "certain number of warps, each warp with 32 threads.\n",
    "\n",
    "The various generations of NVidia SM's is summarized by a _CUDA Capability_, which for the A100 is 8.0. This is not to be confused with the CUDA version, or other versions. It is an abstracted way of referring to functionality, such as having tensor cores or not. \n",
    "\n",
    "Besides the SM's, the NVidia GPU has various forms of memory, such a _global_, _shared_, _texture_ and _constant_. Each is optimized for a particular access. The texture memory, for instance, is optimized for concurrent read-only access, as needed by texture mapping when rendering graphics. We are most interested in the global memory, because it is basic and simple, with some notations about shared, which can be faster, but must be carefully programmer. \n",
    "\n",
    "----\n",
    "\n",
    "<div style=\"float:right;\">\n",
    "<img src=\"./A100-SM.png\" width=50%> \n",
    "</div>\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc0262e",
   "metadata": {},
   "source": [
    "### Floating Point Data types\n",
    "\n",
    "The digram of the SM shows hardware for several arthemtic standards,\n",
    "\n",
    "- INT32, 32 bit integer\n",
    "- FP32, for IEEE standard 32 bit floating point\n",
    "- FP64, for IEEE standard 64 bit floating point.\n",
    "- TF32, a 32 bit floating point in the FP unit, and 16 bit in the tensor unit\n",
    "\n",
    "The use of 64 bit floating point is specific to CUDA. It is not a focus for graphics programming, and most GPU's do not provide 64-bit floating point. \n",
    "\n",
    "In fact, reduced precision floating-point, below what has ever been used, is popular, such as 16 bit floating point with a choice of BF16 with greater range (exponent bits) or FP16 with greater precision (mantissa bits).\n",
    "\n",
    "----\n",
    "\n",
    "<div style=\"float:right;\">\n",
    "<img src=\"./A100-FPtypes.png\" width=50%> \n",
    "</div>\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6656f84f",
   "metadata": {},
   "source": [
    "\n",
    "### Device Capabilities\n",
    "\n",
    "The program DeviceQuery, found in the samples, queries all found devices for their capabilities. We have 4 A100 GPU's on thoreau, each returning theses capabilities.\n",
    "\n",
    "<pre>\n",
    "Device 0: \"NVIDIA A100 80GB PCIe\"\n",
    "  CUDA Driver Version / Runtime Version          11.8 / 11.2\n",
    "  CUDA Capability Major/Minor version number:    8.0\n",
    "  Total amount of global memory:                 81100 MBytes (85039775744 bytes)\n",
    "  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores\n",
    "  GPU Max Clock rate:                            1410 MHz (1.41 GHz)\n",
    "  Memory Clock rate:                             1512 Mhz\n",
    "  Memory Bus Width:                              5120-bit\n",
    "  L2 Cache Size:                                 41943040 bytes\n",
    "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
    "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
    "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
    "  Total amount of constant memory:               65536 bytes\n",
    "  Total amount of shared memory per block:       49152 bytes\n",
    "  Total shared memory per multiprocessor:        167936 bytes\n",
    "  Total number of registers available per block: 65536\n",
    "  Warp size:                                     32\n",
    "  Maximum number of threads per multiprocessor:  2048\n",
    "  Maximum number of threads per block:           1024\n",
    "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
    "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
    "  Maximum memory pitch:                          2147483647 bytes\n",
    "  Texture alignment:                             512 bytes\n",
    "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
    "  Run time limit on kernels:                     No\n",
    "  Integrated GPU sharing Host Memory:            No\n",
    "  Support host page-locked memory mapping:       Yes\n",
    "  Alignment requirement for Surfaces:            Yes\n",
    "  Device has ECC support:                        Enabled\n",
    "  Device supports Unified Addressing (UVA):      Yes\n",
    "  Device supports Managed Memory:                Yes\n",
    "  Device supports Compute Preemption:            Yes\n",
    "  Supports Cooperative Kernel Launch:            Yes\n",
    "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
    "  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n",
    "  Compute Mode:\n",
    "     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0fe4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
